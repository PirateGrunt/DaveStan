---
title: "Poisson Pareto Excess Model"
---

```{r}
library(dplyr)
library(ggplot2)
library(rstan)
```

The data is taken from some short article that Dave sent me. There are just a handful of claims over 7 years. The article seems to refer to a larger set, but this will do for illustrative purposes.

```{r }
attachment <- 500

dfClaims <- data.frame(Year = c(1994, 1995, 1996, 1996, 1997, 1998, 1999, 2000, 2000)
                       , Severity = c(500, 350, 350, 500, 300, 600, 300, 400, 350)) %>% 
  mutate(Excess = pmax(Severity - attachment, 0)
         , ExcessCount = ifelse(Excess > 0, 1, 0))

dfYear <- dfClaims %>% 
  group_by(Year) %>% 
  summarise(AggSeverity = sum(Severity)
            , AggXS = sum(Excess)
            , Frequency = n()
            , ExcessFrequency = sum(ExcessCount))
```

## Fit frequency only

Based on the data, we'll give a very rough idea of the prior distribution for the poisson parameter. We'll use a gamma for this. Below, is a plot of our prior.

```{r }
# Frequency only
freqRate <- 28
freqShape <- 36
pltPriorFreq <- ggplot(data.frame(x = c(0, 4)), aes(x))
pltPriorFreq <- pltPriorFreq + stat_function(fun = dgamma
                                             , args = list(shape = freqShape, rate = freqRate)
                                             , geom = "line")
pltPriorFreq
```

And, after we run a stan model ...

```{r }
fitPois <- stan(file = 'Poisson.stan'
                , data = list(numYears = length(dfYear$Year)
                              , Frequency = dfYear$Frequency
                              , freqShape = freqShape
                              , freqRate = freqRate)
                , iter = 1000
                , seed = 1234)
```

... here's our posterior frequency distribution.

```{r}
postFreq <- extract(fitPois, 'lambda') %>% unlist()
pltPostFreq <- ggplot(data.frame(postFreq), aes(postFreq)) + geom_density()
pltPostFreq
```

And how many claims would we predict? About this many:

```{r }
claims <- extract(fitPois, 'proj_count') %>% unlist()
pltClaims <- ggplot(as.data.frame(claims), aes(claims)) + geom_bar(fill = "grey")
pltClaims
summary(claims)
```

## Fit pareto

We'll cheat a bit and use an exponential distribution for the severity. With a gamma prior, that gives us a Pareto type II. Or, at least that's what Markus Gesmann says: http://www.magesblog.com/2015/05/posterior-predictive-output-with-stan.html.

```{r}
sevRate <- 0.5
sevShape <- 200
pltPriorSev <- ggplot(data.frame(x = c(0, 800)), aes(x))
pltPriorSev <- pltPriorSev + stat_function(fun = dgamma
                                             , args = list(shape = sevShape, rate = sevRate)
                                             , geom = "line")
pltPriorSev
```

I've created two stan models for the severity. In one, I'm just fitting the severity. In another, I calculate an aggregate amount based on a fixed parameter for the number of claims. (More on this below.) In each case, I can also calculate the xs claim amount given an attachment point. This is pretty trivial and something I could easily have done in R once I get FGU results back.

```{r}
fitPareto <- stan(file = 'Pareto.stan'
                , data = list(numClaims = nrow(dfClaims)
                              , Severity = dfClaims$Severity
                              , sevShape = sevShape
                              , sevRate = sevRate
                              , attachment = attachment)
                , iter = 1000
                , seed = 1234)
```

And here's what some output looks like. We'll see that the excess severity is highly skewed. I'm no expert, but I'd probably want to sample some more values.

```{r}
sevTheta <- extract(fitPareto, 'sevTheta') %>% unlist()
summary(sevTheta)

proj_severity <- extract(fitPareto, 'proj_severity') %>% unlist()
pltSeverity <- ggplot(data.frame(proj_severity), aes(proj_severity)) + geom_density(fill = "grey")
pltSeverity
summary(proj_severity)

xs_severity <- extract(fitPareto, 'xs_severity') %>% unlist()

pltXS <- ggplot(as.data.frame(xs_severity), aes(xs_severity)) + geom_density(fill = "grey")
pltXS
summary(xs_severity)

tail(sort(xs_severity))
```

For the aggregate, we'll use a scalar value of 10 for the number of new claims. This will allow us to declare a variable to house all the claims. With this, we can calculate the aggregate XS within Stan itself.

```{r}
fitParetoAgg <- stan(file = 'ParetoAgg.stan'
                , data = list(numClaims = nrow(dfClaims)
                              , Severity = dfClaims$Severity
                              , sevShape = sevShape
                              , sevRate = sevRate
                              , newClaims = 10
                              , attachment = attachment)
                , iter = 1000
                , seed = 1234)
```

```{r}
agg_xs <- extract(fitParetoAgg, 'agg_xs') %>% unlist()
summary(agg_xs)
pltAggXS <- ggplot(as.data.frame(agg_xs), aes(agg_xs)) + geom_density(fill = "grey")
pltAggXS
```

## Collective risk model

It'd be real cool if we could do a collective risk model, where the number of claims is a random variable. If we try the stan code below, we'll have a problem.

```{r error=TRUE}
fitPP <- stan(file = 'PoissonPareto.stan'
                , data = list(numClaims = nrow(dfClaims)
                              , Severity = dfClaims$Severity
                              , sevShape = sevShape
                              , sevRate = sevRate
                              , attachment = attachment)
                , iter = 1000
                , seed = 1234)
```


We'll get an error like the one below.

```
non-data variables not allowed in dimension declarations.
```

This is because we can't declare a vector with a random length.

We've got a few options here. For me, the easiest is to model things in two steps. First, we'll generate the ground-up claim count. With that, we'll build a vector of claims that's sufficiently large to ensure that we get enough claims for the aggregate. We've already done this before, so we can just pick up the results of our Poisson model. 

```{r}
proj_severity <- extract(fitPareto, 'proj_severity') %>% unlist()
proj_severity <- proj_severity[1:(sum(claims))]

sim <- mapply(function(simNum, numClaims){
 if (numClaims == 0) simNum 
}, simNum = seq.int(length(claims)), numClaims = claims) %>% unlist()

dfSim <- data.frame()
```

As a second option, we can use the ParetoAgg stan model to give us a matrix of claims. We need to make sure that the `newClaims` variable in our Pareto model is at least as large as the max of the frequency results from Poisson. In this case it is, but let's run it again to be sure.

```{r}
fitParetoAgg <- stan(file = 'ParetoAgg.stan'
                      , data = list(numClaims = nrow(dfClaims)
                                    , Severity = dfClaims$Severity
                                    , sevShape = sevShape
                                    , sevRate = sevRate
                                    , newClaims = max(claims)
                                    , attachment = attachment)
                      , iter = 1000
                      , seed = 1234)
```

We'll now have a matrix of claims. We can translate this into an aggregate distribution in one of a few ways. Here's the most straightforsward to program, if not the most efficient. We'll create a list object wherein each item has two elements: the number of claims and a vector of amounts. We can summarize the list pretty easily.

```{r}
dfSeverity <- extract(fitParetoAgg, 'proj_severity')[[1]] %>% as.matrix()
# colnames(dfSeverity) <- seq.int(ncol(dfSeverity))
```

```{r}
lstSim <- vector(mode = "list", length(claims))
lstItem <- list()
for (i in seq_along(claims)){
  lstItem$Frequency <- claims[i]
  if (claims[i] > 0){
    lstItem$Severity <- dfSeverity[i, seq.int(claims[i])]
    lstItem$XS_Severity <- pmax(lstItem$Severity - attachment, 0)
    lstItem$AggXS <- sum(lstItem$XS_Severity)
  } else {
    lstItem$Severity <- NA_real_
    lstItem$XS_Severity <- NA_real_
    lstItem$AggXS <- 0
  }
  lstSim[[i]] <- lstItem
}

agg_xs <- sapply(lstSim, "[[", "AggXS")
summary(agg_xs)
pltAggXS <- ggplot(as.data.frame(agg_xs), aes(agg_xs)) + geom_density(fill = "grey")
pltAggXS
```

```{r eval=FALSE}
dfSeverity$Sim <- seq.int(nrow(dfSeverity))
dfSeverity <- tidyr::gather(dfSeverity, key = Sim, -Sim)
```

A third and, kinda complicated, way to do it would be to get the Agg stan model as many times as we have potential claims. We'd get an aggregate amount for each run which we could weight by the probability of each frequency.

A fourth option could be to create a function within Stan.